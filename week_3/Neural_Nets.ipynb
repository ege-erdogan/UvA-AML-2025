{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üìò Applied Machine Learning - Week 3\n\n**Neural Networks and Convolutional Neural Networks**\n\n---\n\n## üöÄ Before You Start\n\n### ‚öôÔ∏è Setup Requirements\n\n1. **Copy Week 2 code** - Create a file `blocks.py` in this folder containing your week 2 implementations\n2. **Use NumPy only** - All functions must be implemented using [**NumPy**](https://docs.scipy.org/doc/)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìö Overview\n\nThis assignment provides the **building blocks** for **Neural Networks (NNs)**. You'll learn:\n\n- **Fully-connected (Dense) Neural Networks** - Basic network architecture\n- **Convolutional Neural Networks (CNNs)** - Image processing networks\n- **Optimization Methods** - Training neural networks\n- **Image Filtering** - Matrix convolution fundamentals\n\nYou'll implement these components from scratch to understand how modern deep learning frameworks work.\n\n---\n\n### üìù Note\n\nSome concepts may not have been covered in lectures yet. These will be discussed in upcoming sessions."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìë Table of Contents\n\n1. [**Fully-Connected Neural Networks**](#1.-Fully-Connected-Neural-Networks)\n   - [1.1 Dense Layer](#1.1-Dense-layer)\n   - [1.2 ReLU Nonlinearity](#1.2-ReLU-nonlinearity)\n   - [1.3 Sigmoid Nonlinearity](#1.3-Sigmoid-nonlinearity)\n   - [1.4 Sequential Model](#1.4-Sequential-model)\n   - [1.5 NLL Loss Function](#1.5-NLL-loss-function)\n   - [1.6 L‚ÇÇ Regularization](#1.6-$L_2$-regularization)\n   - [1.7 SGD Optimizer](#1.7-SGD-optimizer)\n2. [**Experiments**](#2.-Experiments)\n3. [**Convolutions**](#3.-Convolutions)\n   - [3.1 Matrix Convolution](#3.1-Matrix-convolution)\n   - [3.2 Basic Kernels](#3.2-Basic-kernels)\n   - [3.3 Convolutional Layer](#3.3-Convolutional-layer)\n   - [3.4 Pooling Layer](#3.4-Pooling-layer)\n   - [3.5 Flatten](#3.5-Flatten)\n4. [**Image Experiments**](#4.-Image-Experiments)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import blocks\n",
    "\n",
    "import automark as am\n",
    "\n",
    "# fill in you student number as your username\n",
    "username = \"Your Username\"\n",
    "\n",
    "# to check your progress, you can run this function\n",
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Fully-Connected Neural Networks\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üß† Neural Network Architecture\n\n**Layer composition:** Each layer is a function with parameters (weights):\n\n$$\nh = f(x, w)\n$$\n\nwhere:\n- $h$ = layer output\n- $x$ = input vector\n- $w$ = weight vector\n\n**Network as function composition:**\n\nNeural networks chain layers together:\n\n$$\nF = f_k \\circ f_{k-1} \\circ \\dots \\circ f_1\n$$\n\n$$\n\\begin{align}\nh_1 &= f_1(x, w_1) \\\\\nh_2 &= f_2(h_1, w_2) \\\\\n&\\vdots \\\\\n\\dot{y} &= f_k(h_{k-1}, w_k)\n\\end{align}\n$$\n\n**Parameters:** Different weight vectors $(w_1, w_2, \\ldots, w_k)$ determine the effect of each layer.\n\n> **Note:** \"Weights\" are sometimes called \"parameters\", often denoted as $\\theta$.\n\n---\n\n### üìâ Loss Functions\n\n**Purpose:** Measure neural network performance\n\n**For classification:** Compare predictions with correct values\n\n**Example - Squared Loss:**\n\n$$\n\\mathcal{L} = \\tfrac{1}{2}\\sum_{n = 1}^N (y_n - \\dot{y}_n)^2\n$$\n\nwhere:\n- $n$ = data point index\n- $y_n$ = true value\n- $\\dot{y}_n$ = predicted value\n\n**Training objective:** **Minimize the loss function**\n\n**Training method:** [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)\n\n> **Focus:** This assignment covers the **forward pass**. Backpropagation is implemented for you."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.1 Dense Layer\n\n**Also known as:** Fully-connected layer, Multiplicative layer\n\n**Function:** Transforms input from one dimension to another\n\n$$\nH = XW + b\n$$\n\nwhere:\n- $H$ = layer output\n- $X$ = input matrix of size `(n_objects, d_in)`\n- $W$ = weight matrix of size `(d_in, d_out)`\n- $b$ = bias vector of size `(d_out,)`\n\n**Element-wise formula:**\n\n$$\nH_{nk} = \\sum\\limits_{i=1}^{d_{in}} X_{ni}W_{ik} + b_k\n$$\n\nwhere:\n- $n$ = data object index\n- $k$ = $k^{th}$ output dimension\n\n**Example:**\n\nSingle-layer network classifying 3D points as -1 or 1:\n- Training set: 75 objects\n- $X$ shape: `75 √ó 3`\n- $H$ shape: `75 √ó 1`  \n- $W$ shape: `3 √ó 1`\n\n> **Note:** \"Dense Layer\" is the same as \"Linear\" from week 2, but `n_out` is not restricted to 1."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w3_dense_forward(x_input, W, b):\n",
    "    \"\"\"Perform the mapping of the input\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the output of a dense layer\n",
    "        np.array of size `(n_objects, n_out)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w3_dense_forward, [\"x_input\", \"W\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üîô Backward Pass (Gradient Computation)\n\nThe backward pass computes gradients using the chain rule:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_grad_input(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of\n",
    "        the loss with respect to the input of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with\n",
    "            respect to the ouput of the dense layer\n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss with\n",
    "        respect to the input of the layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    grad_input = grad_output.dot(W.T)\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Computing gradients with respect to weights and bias:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_grad_W(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of\n",
    "        the loss with respect to W parameter of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with\n",
    "            respect to the ouput of the dense layer\n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss\n",
    "        with respect to W parameter of the layer\n",
    "        np.array of size `(n_in, n_out)`\n",
    "    \"\"\"\n",
    "    grad_W = x_input.T.dot(grad_output)\n",
    "    return grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_grad_b(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of\n",
    "        the loss with respect to b parameter of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with\n",
    "            respect to the ouput of the dense layer\n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss\n",
    "        with respect to b parameter of the layer\n",
    "        np.array of size `(n_out,)`\n",
    "    \"\"\"\n",
    "    grad_b = grad_output.sum(0)\n",
    "    return grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üèóÔ∏è Dense Layer Class\n\nBelow is the complete implementation based on your functions above (already implemented for you):"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training_phase = True\n",
    "        self.output = 0.0\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        self.output = x_input\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x_input, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "    def get_params(self):\n",
    "        return []\n",
    "\n",
    "    def get_params_gradients(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(Dense, self).__init__()\n",
    "        # Randomly initializing the weights from normal distribution\n",
    "        self.W = np.random.normal(scale=0.01, size=(n_input, n_output))\n",
    "        self.grad_W = np.zeros_like(self.W)\n",
    "        # initializing the bias with zero\n",
    "        self.b = np.zeros(n_output)\n",
    "        self.grad_b = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        self.output = w3_dense_forward(x_input, self.W, self.b)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x_input, grad_output):\n",
    "        # get gradients of weights\n",
    "        self.grad_W = dense_grad_W(x_input, grad_output, self.W, self.b)\n",
    "        self.grad_b = dense_grad_b(x_input, grad_output, self.W, self.b)\n",
    "        # propagate the gradient backwards\n",
    "        return dense_grad_input(x_input, grad_output, self.W, self.b)\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def get_params_gradients(self):\n",
    "        return [self.grad_W, self.grad_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer = Dense(2, 1)\n",
    "x_input = np.random.random((3, 2))\n",
    "y_output = dense_layer.forward(x_input)\n",
    "print(x_input)\n",
    "print(y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.2 ReLU Nonlinearity\n\n**Why nonlinearity?** Combining linear layers is still linear:\n\n$$\n\\begin{align}\nH_1 &= XW_1 + b_1\\\\\nH_2 &= H_1W_2 + b_2\\\\\nH_2 &= (XW_1 + b_1)W_2 + b_2 \\\\\n    &= X(W_1W_2) + (b_1W_2 + b_2) \\\\\n    &= XW^* + b^*\n\\end{align}\n$$\n\n**With nonlinearity:**\n\n$$\n\\begin{align}\nH_1 &= XW_1 + b_1\\\\\nH_2 &= f(H_1)W_2 + b_2\\\\\nH_2 &= f(XW_1 + b_1)W_2 + b_2 \\neq XW^* + b^*\n\\end{align}\n$$\n\n**ReLU (Rectified Linear Unit):**\n\nSimple, popular nonlinear activation function (no trainable weights):\n\n$$\n\\text{ReLU}(x) = \\max(0, x)\n$$\n\n<img src=\"./src/relu.png\" width=\"500\">\n\n**Example:**\n\n$$\n\\text{ReLU} \\left(\n\\begin{bmatrix}\n1 & -0.5 \\\\\n0.3 & 0.1 \n\\end{bmatrix}\n\\right) = \n\\begin{bmatrix}\n1 & 0 \\\\\n0.3 & 0.1 \n\\end{bmatrix}\n$$\n\nImplement the forward pass and backward pass (gradient) for ReLU:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w3_relu_forward(x_input):\n",
    "    \"\"\"relu nonlinearity\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "    # Output\n",
    "        the output of relu layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward pass for ReLU, see example above\n",
    "x_input = np.array([[1, -0.5], [0.3, 0.1]])\n",
    "\n",
    "print(w3_relu_forward(x_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w3_relu_forward, [\"x_input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad_input(x_input, grad_output):\n",
    "    \"\"\"relu nonlinearity gradient.\n",
    "        Calculate the partial derivative of the loss\n",
    "        with respect to the input of the layer\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "            grad_output: np.array of size `(n_objects, n_in)`\n",
    "    # Output\n",
    "        the partial derivative of the loss\n",
    "        with respect to the input of the layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    grad_input = grad_output * (x_input > 0)\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        self.output = w3_relu_forward(x_input)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x_input, grad_output):\n",
    "        return relu_grad_input(x_input, grad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3 Sigmoid Nonlinearity\n\nUsing the sigmoid implementation from week 2:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        self.output = blocks.w2_sigmoid_forward(x_input)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x_input, grad_output):\n",
    "        return blocks.w2_sigmoid_grad_input(x_input, grad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.4 Sequential Model\n\n`SequentialNN` class stores layers and performs basic operations (implemented for you):"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialNN(object):\n",
    "\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = layers\n",
    "        self.training_phase = True\n",
    "\n",
    "    def set_training_phase(self, is_training=True):\n",
    "        self.training_phase = is_training\n",
    "        for layer in self.layers:\n",
    "            layer.training_phase = is_training\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        self.output = x_input\n",
    "        for layer in self.layers:\n",
    "            self.output = layer.forward(self.output)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x_input, grad_output):\n",
    "        inputs = [x_input] + [l.output for l in self.layers[:-1]]\n",
    "        for input_, layer_ in zip(inputs[::-1], self.layers[::-1]):\n",
    "            grad_output = layer_.backward(input_, grad_output)\n",
    "\n",
    "    def get_params(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.get_params())\n",
    "        return params\n",
    "\n",
    "    def get_params_gradients(self):\n",
    "        grads = []\n",
    "        for layer in self.layers:\n",
    "            grads.extend(layer.get_params_gradients())\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üìä Example Network Architecture\n\nSimple neural network taking input of shape `(Any, 10)`:\n\n```\n  INPUT (Any, 10)\n       |\n  Dense(10, 4)\n       |\n     ReLU\n       |\n  Dense(4, 1)\n       |\n   Sigmoid\n       |\n  OUTPUT (Any, 1)\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = SequentialNN(Dense(10, 4), ReLU(), Dense(4, 1), Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.forward(np.ones([2, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.5 NLL Loss Function\n\nLoss functions compute both value and gradient (implemented for you):"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLL(object):\n",
    "\n",
    "    def forward(self, target_pred, target_true):\n",
    "        self.output = blocks.w2_nll_forward(target_pred, target_true)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, target_pred, target_true):\n",
    "        return blocks.w2_nll_grad_input(target_pred, target_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.6 L‚ÇÇ Regularization\n\n**Purpose:** Penalize model complexity to prevent overfitting\n\n**Problem:** Complex models (many parameters, large weights) can overfit training data\n\n**Solution:** Add regularization term to loss function\n\n**L‚ÇÇ Regularization (Weight Decay):**\n\n$$\n\\mathcal{L}^* = \\mathcal{L} + \\frac{\\lambda}{2} \\|w\\|^2_2\n$$\n\nwhere:\n- $\\lambda$ = weight decay (hyperparameter controlling regularization strength)\n- $\\|w\\|^2_2$ = squared [Euclidean norm](https://en.wikipedia.org/wiki/Euclidean_distance)\n\n**Expanded form:**\n\n$$\n\\mathcal{L}^* = \\mathcal{L} + \\frac{\\lambda}{2} \\sum\\limits_{m=1}^k \\|w_m\\|^2_2\n$$\n\n**Modified weight update:**\n\n$$\n\\begin{align}\nw_m &\\leftarrow w_m - \\gamma \\frac{\\partial \\mathcal{L}^*}{\\partial w_m}\\\\\n\\frac{\\partial \\mathcal{L}^*}{\\partial w_m} &= \\frac{\\partial \\mathcal{L}}{\\partial w_m} + \\lambda w_m\\\\\nw_m &\\leftarrow w_m - \\gamma \\left(\\frac{\\partial \\mathcal{L}}{\\partial w_m} + \\lambda w_m\\right)\n\\end{align}\n$$\n\n**Implement L‚ÇÇ regularization:**\n\n$$\nL_2(\\lambda, [w_1, w_2, \\dots, w_k]) = \\frac{\\lambda}{2} \\sum\\limits_{m=1}^k \\|w_m\\|^2_2\n$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w3_l2_regularizer(weight_decay, weights):\n",
    "    \"\"\"Compute the L2 regularization term\n",
    "    # Arguments\n",
    "        weight_decay: float\n",
    "        weights: list of arrays of variable sizes\n",
    "    # Output\n",
    "        L2 regularization term\n",
    "        scalar\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üß™ Test Case\n\nExpected output: `108.25`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the L2 regularizer\n",
    "weight_decay = 2\n",
    "weights = [np.array([5, 3, 7, 5, 0.5])]\n",
    "print(w3_l2_regularizer(weight_decay, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w3_l2_regularizer, [\"weight_decay\", \"weights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.7 SGD Optimizer\n\n**Stochastic Gradient Descent** - optimization algorithm for training neural networks:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent optimizer\n",
    "    https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, lr=0.01, weight_decay=0.0):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def update_params(self):\n",
    "        weights = self.model.get_params()\n",
    "        grads = self.model.get_params_gradients()\n",
    "        for w, dw in zip(weights, grads):\n",
    "            update = self.lr * (dw + self.weight_decay * w)\n",
    "            # it writes the result to the previous variable instead of copying\n",
    "            np.subtract(w, update, out=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Experiments\n\n---\n\nLet's test our neural network on the 2 circles dataset from week 2:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some function from week 2\n",
    "def generate_2_circles(N=100):\n",
    "    phi = np.linspace(0.0, np.pi * 2, 100)\n",
    "    X1 = 1.1 * np.array([np.sin(phi), np.cos(phi)])\n",
    "    X2 = 3.0 * np.array([np.sin(phi), np.cos(phi)])\n",
    "    Y = np.concatenate([np.ones(N), np.zeros(N)]).reshape((-1, 1))\n",
    "    X = np.hstack([X1, X2]).T\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def split(X, Y, train_ratio=0.7):\n",
    "    size = len(X)\n",
    "    train_size = int(size * train_ratio)\n",
    "    indices = np.arange(size)\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "    return X[train_indices], Y[train_indices], X[test_indices], Y[test_indices]\n",
    "\n",
    "\n",
    "def plot_model_prediction(prediction_func, X, Y, hard=True):\n",
    "    u_min = X[:, 0].min() - 1\n",
    "    u_max = X[:, 0].max() + 1\n",
    "    v_min = X[:, 1].min() - 1\n",
    "    v_max = X[:, 1].max() + 1\n",
    "\n",
    "    U, V = np.meshgrid(np.linspace(u_min, u_max, 100), np.linspace(v_min, v_max, 100))\n",
    "    UV = np.stack([U.ravel(), V.ravel()]).T\n",
    "    c = prediction_func(UV).ravel()\n",
    "    if hard:\n",
    "        c = c > 0.5\n",
    "    plt.scatter(UV[:, 0], UV[:, 1], c=c, edgecolors=\"none\", alpha=0.15)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y.ravel(), edgecolors=\"black\")\n",
    "    plt.xlim(left=u_min, right=u_max)\n",
    "    plt.ylim(bottom=v_min, top=v_max)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = split(*generate_2_circles(), 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Training the network ##\n",
    "###YOUR CODE FOR DESIGNING THE NETWORK ###\n",
    "model = SequentialNN(\n",
    "    # 2 -> 16 -> 1 With ReLU and Sigmoid where it is required\n",
    ")\n",
    "\n",
    "\n",
    "loss = NLL()\n",
    "weight_decay = 1e-4\n",
    "sgd = SGD(model, lr=0.1, weight_decay=weight_decay)\n",
    "iters = 5000  # Number of times to iterate over all data objects\n",
    "\n",
    "model.set_training_phase(True)\n",
    "\n",
    "for i in range(iters):\n",
    "    # get the predictions\n",
    "    y_pred = model.forward(X_train)\n",
    "\n",
    "    # compute the loss value + L_2 term\n",
    "    loss_value = loss.forward(y_pred, Y_train) + w3_l2_regularizer(\n",
    "        weight_decay, model.get_params()\n",
    "    )\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        # log the current loss value\n",
    "        print(\"Step: {}, \\tLoss = {:.2f}\".format(i, loss_value))\n",
    "\n",
    "    # get the gradient of the loss functions\n",
    "    loss_grad = loss.backward(y_pred, Y_train)\n",
    "\n",
    "    # backprop the gradients\n",
    "    model.backward(X_train, loss_grad)\n",
    "\n",
    "    # perform the updates\n",
    "    sgd.update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_prediction(lambda x: model.forward(x), X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Convolutions\n\n---\n\n### üî≤ 3.1 Matrix Convolution\n\n**Locally connected layers** learn local correlations using fewer parameters than dense layers.\n\n**Convolutional Layer** is based on **matrix convolution**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üñºÔ∏è Visualizing Convolution\n\nA picture is worth a thousand words:\n\n![Image convolution](./src/conv.png)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üìê Mathematical Definition\n\n**Process:**\n1. A **filter** (or **kernel**) slides over the source matrix\n2. Each kernel element multiplies corresponding source element\n3. Results are summed and written to target matrix\n\n**Zero Padding:**\n- Output is smaller than input (kernel can't overlap borders)\n- Solution: Add border of zeros to maintain dimensions\n- Allows kernel to process edge pixels\n\n**Formula:**\n\nGiven source matrix $X$ of size $N \\times M$ and kernel $K$ of size $(2p+1) \\times (2q+1)$:\n\nDefine $X_{ij} = 0$ for $i > N, i < 1$ and $j > M, j < 1$ (zero padding).\n\n$$\nY = X \\star K\n$$\n\n$$\nY_{ij} = \\sum\\limits_{\\alpha=0}^{2p} \\sum\\limits_{\\beta=0}^{2q}\nK_{\\alpha \\beta} X_{i + \\alpha - p, j+\\beta - q}\n$$\n\n**Terminology:**\n- **Machine Learning:** convolution\n- **Mathematics:** cross-correlation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ‚úèÔ∏è Exercise: Implement Matrix Convolution\n\nNow implement convolution with zero padding:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w3_conv_matrix(matrix, kernel):\n",
    "    \"\"\"Perform the convolution of the matrix\n",
    "        with the kernel using zero padding\n",
    "    # Arguments\n",
    "        matrix: input matrix np.array of size `(N, M)`\n",
    "        kernel: kernel of the convolution\n",
    "            np.array of size `(2p + 1, 2q + 1)`\n",
    "    # Output\n",
    "        the result of the convolution\n",
    "        np.array of size `(N, M)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üß™ Test Case\n\nTest with the following data:\n\n$$\nX = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 3 & 4 \\\\\n3 & 4 & 5 \\\\\n\\end{bmatrix} \\quad\nK = \n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 2 \\\\\n\\end{bmatrix}\n$$\n\n**Expected output:**\n\n$$\nX \\star K = \n\\begin{bmatrix}\n7 & 10 & 3 \\\\\n10 & 14 & 6 \\\\\n3 & 6 & 8 \\\\\n\\end{bmatrix}\n$$\n\n> **Note:** [np.eye](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.eye.html) fills matrix with ones on the diagonal."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2, 3], [2, 3, 4], [3, 4, 5]])\n",
    "\n",
    "K = np.eye(3)\n",
    "K[-1, -1] = 2\n",
    "print(np.zeros(3))\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Compare your result with the expected output above:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w3_conv_matrix(X, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w3_conv_matrix, [\"matrix\", \"kernel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.2 Basic Kernels\n\n**Image processing with convolution** (like Instagram filters): blur, shift, edge detection, etc.\n\n**üìñ Recommended read:** [Interactive Image Kernels Article](http://setosa.io/ev/image-kernels/)\n\n**Predefined Kernels:**\n\nConvolutional layers **learn** kernels through training, but some common kernels exist:\n\n**Sharpen Kernel:** \n$$ \n\\begin{bmatrix}\n0 & -1 & 0 \\\\\n-1 & 5 & -1 \\\\\n0 & -1 & 0 \n\\end{bmatrix}\n$$\n\n**Edge Detection Filter:**\n$$\n\\begin{bmatrix}\n-1 & -1 & -1 \\\\\n-1 & 8 & -1 \\\\\n-1 & -1 & -1 \n\\end{bmatrix}\n$$\n\n**Box Blur (size 3):**\n$$ \\frac{1}{9}\n\\begin{bmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1 \n\\end{bmatrix}\n$$\n\nLet's experiment with a dog image!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = plt.imread(\"./images/dog.png\")\n",
    "plt.imshow(rgb_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Converting to grayscale:**\n\nColored images require 3D tensors (RGB). We convert to grayscale for 2D processing:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = rgb_img.mean(axis=2)\n",
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üì¶ Box Blur Implementation\n\n**[Box blur](https://en.wikipedia.org/wiki/Box_blur)** - convolution with kernel of size $N \\times N$:\n\n$$\n\\frac{1}{N^2}\n\\begin{bmatrix}\n1 & \\dots  & 1\\\\\n\\vdots & \\ddots & \\vdots\\\\\n1 & \\dots  & 1\\\\\n\\end{bmatrix}\n$$\n\n**Interpretation:** Takes the average of an image region\n\n**Arguments:**\n- `image` - Input matrix `np.array` of size `(N, M)`\n- `box_size` - Kernel size `int > 0` (kernel is `(box_size, box_size)`)\n\n**Output:**  \nBlurred image `np.array` of size `(N, M)`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w3_box_blur(image, box_size):\n",
    "    \"\"\"Perform the blur of the image\n",
    "    # Arguments\n",
    "        image: input matrix - np.array of size `(N, M)`\n",
    "        box_size: the size of the blur kernel - int > 0\n",
    "            the kernel is of size `(box_size, box_size)`\n",
    "    # Output\n",
    "        the result of the blur\n",
    "            np.array of size `(N, M)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üß™ Test Case\n\nRunning the code should yield:\n\n$$ \n\\begin{bmatrix}\n1 & 2 & 1 \\\\\n2 & 4 & 2 \\\\\n1 & 2 & 1 \n\\end{bmatrix}\n$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.array([[9, 0, 9], [0, 0, 0], [9, 0, 9]])\n",
    "\n",
    "print(w3_box_blur(test_image, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w3_box_blur, [\"image\", \"box_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üêï Blurring the Dog\n\nApplying box blur to the image:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur_dog = w3_box_blur(img, box_size=3)\n",
    "plt.imshow(blur_dog, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üîç Edge Detection\n\nComputing vertical and horizontal gradients:\n\n$$\nK_h = \n\\begin{bmatrix}\n-1 & 0  & 1\\\\\n\\end{bmatrix} \\quad\nK_v = \n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n-1\\\\\n\\end{bmatrix}\n$$\n\n$$\nX_h = X \\star K_h \\quad X_v = X \\star K_v\n$$\n\n**Gradient amplitude:**\n\n$$\nX_\\text{grad} = \\sqrt{X_h^2 + X_v^2}\n$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_h = w3_conv_matrix(blur_dog, np.array([[-1, 0, 1]]))\n",
    "dog_v = w3_conv_matrix(blur_dog, np.array([[-1, 0, 1]]).T)\n",
    "dog_grad = np.sqrt(dog_h**2 + dog_v**2)\n",
    "plt.imshow(dog_grad, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "This produces edges of the blurred dog. Other edge detection methods include:\n- [Canny edge detection](https://en.wikipedia.org/wiki/Canny_edge_detector)\n- [Sobel operator](https://en.wikipedia.org/wiki/Sobel_operator)\n- [Prewitt operator](https://en.wikipedia.org/wiki/Prewitt_operator)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üéØ Pattern Detection\n\n**Key insight:** Convolving with a kernel produces a **response map**\n\nHigher correlation between image patch and kernel ‚Üí Higher response\n\nLet's detect a cross pattern:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n",
    "# Create the image\n",
    "image = np.pad(pattern, [(12, 12), (10, 14)], mode=\"constant\", constant_values=0)\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.title(\"original image\")\n",
    "plt.show()\n",
    "\n",
    "# Add some noise\n",
    "image = 0.5 * image + 0.5 * np.random.random(image.shape)\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.title(\"noisy image\")\n",
    "plt.show()\n",
    "\n",
    "# Let's find the cross\n",
    "response = w3_conv_matrix(image, pattern)\n",
    "plt.imshow(response, cmap=\"gray\")\n",
    "plt.title(\"local response\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(response == response.max(), cmap=\"gray\")\n",
    "plt.title(\"detected position\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The brightest pixel shows where the cross is located!\n\n**Application:** Find patterns in images (eyes, legs, dogs, cats, etc.)\n\n**Next step:** Instead of hand-crafting kernels, we can **learn** them by minimizing loss - that's what **Convolutional Layers** do!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.3 Convolutional Layer\n\n**Working with images:** 3D tensors of shape $N_{\\text{channels}} \\times H \\times W$\n\n- `channels` = color channels (3 for RGB, 1 for grayscale)\n- $H$ = height\n- $W$ = width\n\n**Batch of images:** 4D tensor of shape $N_{\\text{objects}} \\times N_{\\text{channels}} \\times H \\times W$\n\n**Example:** 32 RGB images of size $224 \\times 224$ ‚Üí shape `(32, 3, 224, 224)`\n\n---\n\n**Convolutional Layer Parameters:**\n\n- **Kernels:** Tensor of size `(n_in, n_out, kernel_h, kernel_w)`\n- **Input:** `(n_objects, n_in, H, W)`\n- **Output:** `(n_objects, n_out, H, W)`\n\n**Process:**\n1. To get 1st output channel: convolve all input channels with their kernels\n2. Sum the results\n3. Write to output channel\n\n**Pseudocode:**\n```python\nfor i in range(n_out):\n    out_channel = 0.0\n    for j in range(n_in):\n        kernel_2d = K[i, j]\n        input_channel = input_image[j]\n        out_channel += conv_matrix(input_channel, kernel_2d)\n    output_image.append(out_channel)\n```\n\n> **Implementation:** Provided for you. Backward pass uses matrix multiplication representation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(Layer):\n",
    "    \"\"\"\n",
    "    Convolutional Layer. The implementation is based on\n",
    "        the representation of the convolution as matrix multiplication\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_out, filter_size):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.W = np.random.normal(size=(n_out, n_in, filter_size, filter_size))\n",
    "        self.b = np.zeros(n_out)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        n_obj, n_in, h, w = x_input.shape\n",
    "        n_out = len(self.W)\n",
    "\n",
    "        self.output = []\n",
    "\n",
    "        for image in x_input:\n",
    "            output_image = []\n",
    "            for i in range(n_out):\n",
    "                out_channel = 0.0\n",
    "                for j in range(n_in):\n",
    "                    out_channel += w3_conv_matrix(image[j], self.W[i, j])\n",
    "                output_image.append(out_channel)\n",
    "            self.output.append(np.stack(output_image, 0))\n",
    "\n",
    "        self.output = np.stack(self.output, 0)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x_input, grad_output):\n",
    "\n",
    "        N, C, H, W = x_input.shape\n",
    "        F, C, HH, WW = self.W.shape\n",
    "\n",
    "        pad = int((HH - 1) / 2)\n",
    "\n",
    "        self.grad_b = np.sum(grad_output, (0, 2, 3))\n",
    "\n",
    "        # pad input array\n",
    "        x_padded = np.pad(x_input, ((0, 0), (0, 0), (pad, pad), (pad, pad)), \"constant\")\n",
    "        H_padded, W_padded = x_padded.shape[2], x_padded.shape[3]\n",
    "        # naive implementation of im2col\n",
    "        x_cols = None\n",
    "        for i in range(HH, H_padded + 1):\n",
    "            for j in range(WW, W_padded + 1):\n",
    "                for n in range(N):\n",
    "                    field = x_padded[n, :, i - HH : i, j - WW : j].reshape((1, -1))\n",
    "                    if x_cols is None:\n",
    "                        x_cols = field\n",
    "                    else:\n",
    "                        x_cols = np.vstack((x_cols, field))\n",
    "\n",
    "        x_cols = x_cols.T\n",
    "\n",
    "        d_out = grad_output.transpose(1, 2, 3, 0)\n",
    "        dout_cols = d_out.reshape(F, -1)\n",
    "\n",
    "        dw_cols = np.dot(dout_cols, x_cols.T)\n",
    "        self.grad_W = dw_cols.reshape(F, C, HH, WW)\n",
    "\n",
    "        w_cols = self.W.reshape(F, -1)\n",
    "        dx_cols = np.dot(w_cols.T, dout_cols)\n",
    "\n",
    "        dx_padded = np.zeros((N, C, H_padded, W_padded))\n",
    "        idx = 0\n",
    "        for i in range(HH, H_padded + 1):\n",
    "            for j in range(WW, W_padded + 1):\n",
    "                for n in range(N):\n",
    "                    dx_padded[n : n + 1, :, i - HH : i, j - WW : j] += dx_cols[\n",
    "                        :, idx\n",
    "                    ].reshape((1, C, HH, WW))\n",
    "                    idx += 1\n",
    "            dx = dx_padded[:, :, pad:-pad, pad:-pad]\n",
    "        grad_input = dx\n",
    "        return grad_input\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def get_params_gradients(self):\n",
    "        return [self.grad_W, self.grad_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Example:** Transform 3-channel images to 8-channel images using `3√ó3` kernels:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = ConvLayer(3, 8, filter_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.4 Pooling Layer\n\n**Purpose:** Reduce image size (downsampling)\n\n**Max Pooling:** Most common pooling operation\n\n![Pooling](./src/pool.png)\n\n**Effect:**\n- Reduces spatial dimensions by half (with $2 \\times 2$ windows)\n- **No effect on depth** (number of channels)\n\n**Process:**\n- Split image into windows\n- Take maximum value from each window\n- Use as output\n\n![Max Pooling](./src/maxpool.png)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w3_maxpool_forward(x_input):\n",
    "    \"\"\"Perform max pooling operation with 2x2 window\n",
    "    # Arguments\n",
    "        x_input: np.array of size (2 * W, 2 * H)\n",
    "    # Output\n",
    "        output: np.array of size (W, H)\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üß™ Test Case\n\n**Input:**\n$$ \n\\begin{bmatrix}\n1 & 1 & 2 & 4 \\\\\n5 & 6 & 7 & 8 \\\\\n3 & 2 & 1 & 0 \\\\\n1 & 2 & 3 & 4\n\\end{bmatrix}\n$$\n\n**Expected output:**\n$$ \n\\begin{bmatrix}\n6 & 8 \\\\\n3 & 4\n\\end{bmatrix}\n$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.array([[1, 1, 2, 4], [5, 6, 7, 8], [3, 2, 1, 0], [1, 2, 3, 4]])\n",
    "\n",
    "print(w3_maxpool_forward(test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w3_maxpool_forward, [\"x_input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üîô Gradient Implementation\n\nBackward pass already implemented. Read the code to understand the concept:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool_grad_input(x_input, grad_output):\n",
    "    \"\"\"Calculate partial derivative of the loss with respect to the input\n",
    "    # Arguments\n",
    "        x_input: np.array of size (2 * W, 2 * H)\n",
    "        grad_output: partial derivative of the loss\n",
    "            with respect to the output\n",
    "            np.array of size (W, H)\n",
    "    # Output\n",
    "        output: partial derivative of the loss\n",
    "            with respect to the input\n",
    "            np.array of size (2 * W, 2 * H)\n",
    "    \"\"\"\n",
    "    height, width = x_input.shape\n",
    "    # create the array of zeros of the required size\n",
    "    grad_input = np.zeros(x_input.shape)\n",
    "\n",
    "    # let's put 1 if the element with this position\n",
    "    # is maximal in the window\n",
    "    for i in range(0, height, 2):\n",
    "        for j in range(0, width, 2):\n",
    "            window = x_input[i : i + 2, j : j + 2]\n",
    "            i_max, j_max = np.unravel_index(np.argmax(window), (2, 2))\n",
    "            grad_input[i + i_max, j + j_max] = 1\n",
    "\n",
    "    # put corresponding gradient instead of 1\n",
    "    grad_input = grad_input.ravel()\n",
    "    grad_input[grad_input == 1] = grad_output.ravel()\n",
    "    grad_input = grad_input.reshape(x_input.shape)\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Complete MaxPool2x2 Layer implementation:**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2x2(Layer):\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        n_obj, n_ch, h, w = x_input.shape\n",
    "        self.output = np.zeros((n_obj, n_ch, h // 2, w // 2))\n",
    "        for i in range(n_obj):\n",
    "            for j in range(n_ch):\n",
    "                self.output[i, j] = w3_maxpool_forward(x_input[i, j])\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x_input, grad_output):\n",
    "        n_obj, n_ch, _, _ = x_input.shape\n",
    "        grad_input = np.zeros_like(x_input)\n",
    "        for i in range(n_obj):\n",
    "            for j in range(n_ch):\n",
    "                grad_input[i, j] = maxpool_grad_input(x_input[i, j], grad_output[i, j])\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.5 Flatten\n\n**Purpose:** Bridge convolutional and dense layers\n\n**Problem:**\n- Convolutional layers work with 4D tensors\n- Dense layers work with 2D matrices (matrices)\n\n**Solution:** Flatten layer reshapes tensors\n\n**Transformation:**\n- **Input:** `(n_obj, n_channels, h, w)`\n- **Output:** `(n_obj, n_channels * h * w)`\n\n**Backward pass:** Simply reshape back (no value changes)\n\n**Implementation:** Use [np.reshape](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.reshape.html)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w3_flatten_forward(x_input):\n",
    "    \"\"\"Perform the reshaping of the tensor of size `(K, L, M, N)`\n",
    "        to the tensor of size `(K, L*M*N)`\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(K, L, M, N)`\n",
    "    # Output\n",
    "        output: np.array of size `(K, L*M*N)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üß™ Test Case\n\nExpected shape: `(100, 768)`\n\n> **Note:** We use [np.zeros](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.zeros.html) to test shape transformation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = np.zeros((100, 3, 16, 16))\n",
    "\n",
    "print(w3_flatten_forward(test_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w3_flatten_forward, [\"x_input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_grad_input(x_input, grad_output):\n",
    "    \"\"\"Calculate partial derivative of the loss with respect to the input\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(K, L, M, N)`\n",
    "        grad_output: partial derivative of the loss\n",
    "            with respect to the output\n",
    "            np.array of size `(K, L*M*N)`\n",
    "    # Output\n",
    "        output: partial derivative of the loss\n",
    "            with respect to the input\n",
    "            np.array of size `(K, L, M, N)`\n",
    "    \"\"\"\n",
    "    grad_input = grad_output.reshape(x_input.shape)\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Complete Flatten Layer implementation:**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer(Layer):\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        self.output = w3_flatten_forward(x_input)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x_input, grad_output):\n",
    "        output = flatten_grad_input(x_input, grad_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Image Experiments\n\n---\n\n### üéØ Training with Mini-Batches\n\n**Strategy:** Feed small portions of dataset (mini-batches) one-by-one to the network\n\n**Benefit:** More efficient training and better generalization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def iterate_minibatches(x, y, batch_size=16, verbose=True):\n",
    "    assert len(x) == len(y)\n",
    "\n",
    "    indices = np.arange(len(x))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for i, start_idx in enumerate(range(0, len(x) - batch_size + 1, batch_size)):\n",
    "        if verbose:\n",
    "            print(\"\\rBatch: {}/{}\".format(i + 1, len(x) // batch_size), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        excerpt = indices[start_idx : start_idx + batch_size]\n",
    "        yield x[excerpt], y[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üìä Loading MNIST Dataset\n\n[Download MNIST dataset](http://yann.lecun.com/exdb/mnist/) first.\n\n> **Note:** Unpack downloaded files if you encounter loading errors."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(load_mnist(dataset=\"training\", path=\".\"))\n",
    "train\n",
    "train_images = np.array([im[1] for im in train])\n",
    "train_targets = np.array([im[0] for im in train])\n",
    "# We will train a 0 vs. 1 classifier\n",
    "x_train = train_images[train_targets < 2][:1000]\n",
    "y_train = train_targets[train_targets < 2][:1000]\n",
    "\n",
    "y_train = y_train\n",
    "y_train = y_train.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üî¢ About MNIST\n\n**MNIST dataset:**\n- Grayscale images (single channel)\n- Size: `28√ó28` pixels\n- RGB values: 0-255\n- Total: 784 pixels per image\n\n**Single image visualization:**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0].reshape(28, 28), cmap=\"gray_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üîß Preprocessing\n\n**Steps:**\n1. **Normalize** values to [0, 1] for easier optimization\n2. **Reshape** to add channel dimension `(n_images, 1, 28, 28)`\n\nThe visual appearance remains unchanged:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = x_train.reshape((-1, 1, 28, 28))\n",
    "plt.imshow(x_train[0].reshape(28, 28), cmap=\"gray_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üèóÔ∏è Building a CNN\n\nTraining a simple convolutional neural network:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn():\n",
    "    nn = SequentialNN(\n",
    "        ConvLayer(1, 2, filter_size=3),  # The output is of size [N_obj 2 28 28]\n",
    "        ReLU(),  # The output is of size [N_obj 2 28 28]\n",
    "        MaxPool2x2(),  # The output is of size [N_obj 2 14 14]\n",
    "        ConvLayer(2, 4, filter_size=3),  # The output is of size [N_obj 4 14 14]\n",
    "        ReLU(),  # The output is of size [N_obj 4 14 14]\n",
    "        MaxPool2x2(),  # The output is of size [N_obj 4 7 7]\n",
    "        FlattenLayer(),  # The output is of size [N_obj 196]\n",
    "        Dense(4 * 7 * 7, 8),\n",
    "        ReLU(),\n",
    "        Dense(8, 1),\n",
    "        Sigmoid(),\n",
    "    )\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = get_cnn()\n",
    "loss = NLL()\n",
    "optimizer = SGD(nn, weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will train for about 5 minutes\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "# We will store the results here\n",
    "history = {\"loss\": [], \"accuracy\": []}\n",
    "\n",
    "# `num_epochs` represents the number of iterations\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "    # We perform iteration a one-by-one iteration of the mini-batches\n",
    "    for x_batch, y_batch in iterate_minibatches(x_train, y_train, batch_size):\n",
    "        # Predict the target value\n",
    "        y_pred = nn.forward(x_batch)\n",
    "        # Compute the gradient of the loss\n",
    "        loss_grad = loss.backward(y_pred, y_batch)\n",
    "        # Perform backwards pass\n",
    "        nn.backward(x_batch, loss_grad)\n",
    "        # Update the params\n",
    "        optimizer.update_params()\n",
    "\n",
    "        # Save loss and accuracy values\n",
    "        history[\"loss\"].append(loss.forward(y_pred, y_batch))\n",
    "        prediction_is_correct = (y_pred > 0.5) == (y_batch > 0.5)\n",
    "        history[\"accuracy\"].append(np.mean(prediction_is_correct))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the results to get a better insight\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "ax_1 = plt.subplot()\n",
    "ax_1.plot(history[\"loss\"], c=\"g\", lw=2, label=\"train loss\")\n",
    "ax_1.set_ylabel(\"loss\", fontsize=16)\n",
    "ax_1.set_xlabel(\"#batches\", fontsize=16)\n",
    "\n",
    "ax_2 = plt.twinx(ax_1)\n",
    "ax_2.plot(history[\"accuracy\"], lw=3, label=\"train accuracy\")\n",
    "ax_2.set_ylabel(\"accuracy\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üí≠ Experiments to Try\n\n**Batch size variations:**\n- `batch_size=1` - What happens?\n- `batch_size=1000` - What happens?\n- Does computation speed depend on batch size? Why?\n\n**Number of epochs:**\n- `num_epochs=1` - What happens?\n- `num_epochs=1000` - What happens?\n- How does it affect computation time, resources, and accuracy?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üî¨ Visualizing Activations\n\nLet's visualize intermediate layer activations to understand what the network learns:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_images = x_batch[:2]\n",
    "_ = nn.forward(viz_images)\n",
    "\n",
    "activations = {\n",
    "    \"conv_1\": nn.layers[0].output,\n",
    "    \"relu_1\": nn.layers[1].output,\n",
    "    \"pool_1\": nn.layers[2].output,\n",
    "    \"conv_2\": nn.layers[3].output,\n",
    "    \"relu_2\": nn.layers[4].output,\n",
    "    \"pool_2\": nn.layers[5].output,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(4, 8))\n",
    "\n",
    "ax1.imshow(viz_images[0, 0], cmap=plt.cm.gray_r)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "ax2.imshow(viz_images[1, 0], cmap=plt.cm.gray_r)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of Conv 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv 1\n",
    "f, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations[\"conv_1\"][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"Channel {}\".format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of ReLU 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU 1\n",
    "f, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations[\"relu_1\"][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"Channel {}\".format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of MaxPooling 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling 1\n",
    "f, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations[\"pool_1\"][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"Channel {}\".format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of Conv 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv 2\n",
    "f, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations[\"conv_2\"][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"Channel {}\".format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of ReLU 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU 2\n",
    "f, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations[\"relu_2\"][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"Channel {}\".format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of MaxPooling 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling 2\n",
    "f, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations[\"pool_2\"][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"Channel {}\".format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üß† Understanding Deep Features\n\nAs we go deeper in the network:\n- Images become **less locally-correlated** (neighboring pixels less dependent)\n- Features become more **semantically meaningful**\n- Each pixel stores more **useful information** about the object\n- Dense layers at the end analyze these high-level features\n\n### üé® More Experiments to Try\n\n**Architecture variations:**\n- Change number of kernels\n- Vary kernel sizes\n- Add/remove layers\n\nExperiment and observe how performance changes!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}