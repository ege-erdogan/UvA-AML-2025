# Week 3: Neural Networks

## ğŸ“š Overview

This week dives deep into **Neural Networks** - the foundation of modern deep learning. You'll implement convolutional neural networks (CNNs) from scratch and understand how they process images and learn complex patterns.

> **ğŸ““ Main Resource:** See [`Neural_Nets.ipynb`](Neural_Nets.ipynb) for detailed explanations and exercises.

---

## ğŸ¯ Learning Objectives

By the end of this week, you will be able to:

- Understand **neural network architecture** and forward propagation
- Implement **convolutional layers** for image processing
- Build **pooling layers** for spatial downsampling
- Apply **activation functions** (ReLU) and **regularization** (L2)
- Construct **fully connected (dense) layers**
- Implement **flattening** to connect convolutional and dense layers
- Create **complete CNN architectures** for image classification

---

## ğŸ“ Assignment: Week 3

Complete the exercises in [`Neural_Nets.ipynb`](Neural_Nets.ipynb). Each function you implement will be tested for correctness using **automark**.

### ğŸ“‹ Instructions

1. Open the [`Neural_Nets.ipynb`](Neural_Nets.ipynb) notebook
2. Follow the step-by-step instructions
3. Implement the required neural network components
4. Test your implementations using automark

### âœ… Grading

- You are registered using your student number
- Each function is automatically tested for correctness
- If your student number is not registered, contact a TA during class

---

## ğŸ”‘ Key Topics Covered

### **Convolutional Neural Networks (CNNs)**
- Convolutional layers and filters
- Stride and padding concepts
- Feature map computation
- Spatial hierarchies in images

### **Pooling Layers**
- Max pooling for downsampling
- Reducing spatial dimensions
- Translation invariance

### **Activation Functions**
- ReLU (Rectified Linear Unit)
- Non-linearity in neural networks
- Forward pass implementation

### **Dense (Fully Connected) Layers**
- Linear transformations
- Connecting features to outputs
- Classification layers

### **Regularization**
- L2 regularization (weight decay)
- Preventing overfitting
- Regularization penalty computation

### **Network Architecture**
- Flattening multi-dimensional tensors
- Connecting convolutional and dense layers
- End-to-end CNN design

---

## ğŸ’¡ Tips for Success

- **Visualize shapes** - Pay close attention to tensor dimensions at each layer
- **Test incrementally** - Use automark to verify each function as you complete it
- **Understand convolution** - Make sure you grasp how filters slide over images
- **Debug with small examples** - Create simple test cases to verify your logic
- **Ask questions** - Don't hesitate to reach out to TAs if you're stuck

---

## ğŸŒŸ Why Neural Networks?

Neural networks, especially CNNs, have revolutionized computer vision:
- **Image classification** - Recognizing objects in photos
- **Object detection** - Locating objects in scenes
- **Medical imaging** - Detecting diseases in X-rays and MRIs
- **Autonomous vehicles** - Understanding road scenes
- **Face recognition** - Identifying individuals

Understanding how to build these networks from scratch gives you deep insight into modern deep learning frameworks like PyTorch and TensorFlow.

---

**Enjoy coding!** ğŸš€