# Week 2: Machine Learning Fundamentals

## ğŸ“š Overview

This week introduces core machine learning algorithms and concepts. You'll implement and understand fundamental classification and regression techniques used throughout the course.

> **ğŸ““ Main Resource:** See [`ML.ipynb`](ML.ipynb) for detailed explanations and exercises.

---

## ğŸ¯ Learning Objectives

By the end of this week, you will be able to:

- Implement and understand **k-Nearest Neighbors (k-NN)** algorithm
- Build **Decision Trees** from scratch
- Understand **logistic regression** and the sigmoid function
- Apply **gradient descent** for optimization
- Evaluate classifier performance

---

## ğŸ“ Assignment: Week 2

Complete the exercises in [`ML.ipynb`](ML.ipynb). Each function you implement will be tested for correctness using **automark**.

### ğŸ“‹ Instructions

1. Open the [`ML.ipynb`](ML.ipynb) notebook
2. Follow the step-by-step instructions
3. Implement the required functions
4. Test your implementations using automark

### âœ… Grading

- You are registered using your student number
- Each function is automatically tested for correctness
- If your student number is not registered, contact a TA during class

---

## ğŸ”‘ Key Topics Covered

### **k-Nearest Neighbors (k-NN)**
- Distance metrics
- Classification based on nearest neighbors
- Implementation from scratch

### **Decision Trees**
- Entropy and information gain
- Splitting criteria
- Tree construction and traversal
- Terminal node predictions

### **Logistic Regression**
- Sigmoid activation function
- Negative log-likelihood loss
- Gradient computation
- Linear transformations

### **Gradient Descent**
- Parameter optimization
- Forward and backward passes
- Weight and bias gradients

---

## ğŸ’¡ Tips for Success

- **Read carefully** - Understand what each function should do before coding
- **Test incrementally** - Use automark to verify each function as you complete it
- **Ask questions** - Don't hesitate to reach out to TAs if you're stuck
- **Experiment** - Try different approaches and learn from errors

---

**Enjoy coding!** ğŸš€