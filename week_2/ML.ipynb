{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 📘 Applied Machine Learning - Week 2\n\n**Machine Learning Fundamentals: Logistic Regression, Nearest Neighbors, and Decision Trees**\n\n---\n\n## 📚 Overview\n\nThis assignment introduces fundamental machine learning algorithms. You'll implement logistic regression, nearest neighbors, and decision trees using **NumPy** only.\n\n## 📑 Table of Contents\n\n1. [**Logistic Regression**](#1.-Logistic-Regression)\n   - [1.1 Linear Mapping](#1.1-Linear-Mapping)\n   - [1.2 Sigmoid](#1.2-Sigmoid)\n   - [1.3 Negative Log Likelihood](#1.3-Negative-Log-Likelihood)\n   - [1.4 Model](#1.4-Model)\n   - [1.5 Simple Experiment](#1.5-Simple-Experiment)\n2. [**Nearest Neighbors**](#2.-Nearest-Neighbors)\n   - [2.1 Distance to Training Samples](#2.1-Distance-of-input-to-training-samples)\n   - [2.2 Predicting a Label](#2.2-Predicting-a-label)\n   - [2.3 Experiment](#2.3-Experiment)\n3. [**Decision Tree**](#3.-Decision-Tree)\n   - [3.1 Entropy & Data Split](#3.1-Entropy-&-Data-Split)\n   - [3.2 Terminal Node](#3.2-Terminal-Node)\n   - [3.3 Build the Decision Tree](#3.3-Build-the-Decision-Tree)\n4. [**Experiments**](#4.-Experiments)\n   - [4.1 Decision Tree for Heart Disease Prediction](#4.1-Decision-Tree-for-Heart-Disease-Prediction)\n   - [4.2 Nearest Neighbors for Heart Disease Prediction](#4.2-Nearest-Neighbors-for-Heart-Disease-Prediction)\n   - [4.3 Logistic Regression for Heart Disease Prediction](#4.3-Logistic-Regression-for-Heart-Disease-Prediction)\n\n---\n\n### 📝 Note\n\nSome concepts below may not have been covered in lectures yet. These will be discussed in upcoming sessions."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 🎯 Before You Begin\n\nTo verify your code, we use **automark**. You're registered with your student number as your username."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automark as am\n",
    "\n",
    "# fill in you student number as your username\n",
    "username = \"Your Username\"\n",
    "\n",
    "# to check your progress, you can run this function\n",
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ✅ Progress Tracking\n\nInitially, all tests show 'not attempted'. By the end of this notebook, you should complete all week 2 tests. Example output:\n\n```\n---------------------------------------------\n| Your name / student number                |\n| your_email@your_domain.whatever           |\n---------------------------------------------\n| Current Assignment Grade  0%              |\n---------------------------------------------\n| w2_linear_forward        | not attempted  |\n| w2_linear_grad_W         | not attempted  |\n| w2_linear_grad_b         | not attempted  |\n| w2_nll_forward           | not attempted  |\n| w2_nll_grad_input        | not attempted  |\n| w2_sigmoid_forward       | not attempted  |\n| w2_sigmoid_grad_input    | not attempted  |\n| w2_dist_to_training_samples | not attempted  |\n| w2_nearest_neighbors     | not attempted  |\n| w2_tree_weighted_entropy | not attempted  |\n| w2_tree_split_data_left  | not attempted  |\n| w2_tree_split_data_right | not attempted  |\n| w2_tree_to_terminal      | not attempted  |\n---------------------------------------------\n```"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ⚠️ Important Note\n\n**Multi-week tracking:** The `am.get_progress(username)` function checks weeks 1, 2, and 3. \n\n- **Week 2 exercises** (this notebook): Linear forward, Linear dW, Linear db, Sigmoid forward, Sigmoid grad, NLL forward, NLL grad, Distance to training samples, Nearest neighbors, Tree entropy, Tree left split, Tree right split, Tree terminal\n- **Week 1 and 3** exercises appear in their respective notebooks\n- Don't worry if week 1 or 3 tests remain \"not attempted\" - focus on week 2 for now\n- All statuses should show \"completed\" when you finish all three weeks"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import (\n",
    "    print_function,\n",
    "    absolute_import,\n",
    "    division,\n",
    ")  # You don't need to know what this is.\n",
    "import numpy as np  # this imports numpy, which is used for vector- and matrix calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 💡 About Classes in Python\n\nThis notebook uses **classes** and **instances** (already implemented). This makes code cleaner and more readable.\n\n**Learning resources:**\n- [Official Python Documentation](https://docs.python.org/3/tutorial/classes.html) \n- Video: [Object Oriented Programming Introduction](https://www.youtube.com/watch?v=ekA6hvk-8H8) by *sentdex*\n- Advanced: [Stop Writing Classes](https://www.youtube.com/watch?v=o9pEzgHorH0) - OOP antipatterns"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Logistic Regression\n\n---\n\n### 📐 Introduction\n\n**Logistic Regression** is a generalized linear model for binary (2-class) classification.\n\n- Can be extended to multi-class and non-linear cases\n- We focus on the simplest binary case here\n\n**The Problem:**\nGiven data with 2 classes (Class 0 and Class 1), logistic regression returns a probability value in [0, 1] representing the likelihood of belonging to Class 1.\n\n**Decision Boundary:**\nThe set of points where prediction = 0.5 forms a line (in 2D) or hyperplane (in higher dimensions) that separates the classes.\n\n![Linear Separability](https://nlpforhackers.io/wp-content/uploads/2018/07/Linear-Separability-610x610.png)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 🧮 Mathematical Formulation\n\n**Model Parameters:**\n- Weight vector **W**\n- Bias **b**\n\n**Prediction for feature vector X:**\n\n$$\nf(X) = \\frac{1}{1 + \\exp(-[XW + b])} = \\sigma(h(X))\n$$\n\nwhere:\n- $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ (sigmoid function)\n- $h(X) = XW + b$ (linear transformation)\n\n**Training Objective:**\nFit W and b by minimizing the **Negative Log-Likelihood (NLL)** on training data $\\{X_j, Y_j\\}_{j=1}^N$:\n\n$$\n\\mathcal{L} = -\\frac{1}{N}\\sum_j \\log\\Big[ f(X_j)^{Y_j} \\cdot (1-f(X_j))^{1-Y_j}\\Big]\n$$\n\n$$\n= -\\frac{1}{N}\\sum_j \\Big[ Y_j\\log f(X_j) + (1-Y_j)\\log(1-f(X_j))\\Big]\n$$"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 🔄 Algorithm Steps\n\n**Computing NLL (Forward Pass):**\n\n1. **Linear mapping:** $h = XW + b$\n2. **Sigmoid activation:** $f = \\sigma(h)$\n3. **Calculate NLL:** $\\mathcal{L} = -\\frac{1}{N}\\sum_j \\Big[ Y_j\\log f_j + (1-Y_j)\\log(1-f_j)\\Big]$"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 📉 Gradient Descent & Backpropagation\n\n**Optimization via [Gradient Descent (GD)](https://en.wikipedia.org/wiki/Gradient_descent):**\n\nChoose a learning rate $\\gamma$ and update parameters after each NLL computation:\n\n$$\nW_{\\text{new}} = W_{\\text{old}} - \\gamma \\frac{\\partial \\mathcal{L}}{\\partial W}\n$$\n\n$$\nb_{\\text{new}} = b_{\\text{old}} - \\gamma \\frac{\\partial \\mathcal{L}}{\\partial b}\n$$\n\n**Computing Gradients via [Backpropagation (BP)](https://en.wikipedia.org/wiki/Backpropagation):**\n\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial W} = \n\\frac{\\partial\\mathcal{L}}{\\partial h} \\frac{\\partial h}{\\partial W} =\n\\frac{\\partial\\mathcal{L}}{\\partial f} \\frac{\\partial f}{\\partial h} \\frac{\\partial h}{\\partial W}\n$$\n\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial b} = \n\\frac{\\partial\\mathcal{L}}{\\partial h} \\frac{\\partial h}{\\partial b} =\n\\frac{\\partial\\mathcal{L}}{\\partial f} \\frac{\\partial f}{\\partial h} \\frac{\\partial h}{\\partial b}\n$$"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.1 Linear Mapping\n\nImplement the linear transformation:\n\n$$\nh(X) = XW + b\n$$"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "**📝 Note on dimensionality:**\n- `n_out` = dimensionality of output (number of predictions per input)\n- For logistic regression: `n_out = 1`\n- For future assignments: `n_out > 1` (multi-output cases)\n- **Tip:** Use **NumPy** operations for generic, dimension-agnostic implementations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_linear_forward(x_input, W, b):\n",
    "    \"\"\"Perform the mapping of the input\n",
    "    # Arguments\n",
    "        x_input: input of the linear function - np.array of size `(n_objects, n_in)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the output of the linear function\n",
    "        np.array of size `(n_objects, n_out)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 🧪 Test Case\n\nLet's verify with matrices $X$, $W$, and $b$:\n\n$$\nX = \\begin{bmatrix}\n1 & -1 \\\\\n-1 & 0 \\\\\n1 & 1 \\\\\n\\end{bmatrix} \\quad\nW = \\begin{bmatrix}\n4 \\\\\n2 \\\\\n\\end{bmatrix} \\quad\nb = \\begin{bmatrix}\n3 \\\\\n\\end{bmatrix}\n$$\n\n**Step 1:** Compute $XW$:\n\n$$\nXW = \\begin{bmatrix}\n1 & -1 \\\\\n-1 & 0 \\\\\n1 & 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n4 \\\\\n2 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 \\\\\n-4 \\\\\n6 \\\\\n\\end{bmatrix}\n$$\n\n**Step 2:** Add bias $b$:\n\n$$\nXW + b = \n\\begin{bmatrix}\n5 \\\\\n-1 \\\\\n9 \\\\\n\\end{bmatrix} \n$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([[1, -1], [-1, 0], [1, 1]])\n",
    "\n",
    "W_test = np.array([[4], [2]])\n",
    "\n",
    "b_test = np.array([3])\n",
    "\n",
    "h_test = w2_linear_forward(X_test, W_test, b_test)\n",
    "print(h_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w2_linear_forward, [\"x_input\", \"W\", \"b\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 📊 Computing Gradients\n\nNow implement the partial derivatives with respect to model parameters:\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W} = \n\\frac{\\partial \\mathcal{L}}{\\partial h}\n\\frac{\\partial h}{\\partial W}\n$$\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b} = \n\\frac{\\partial \\mathcal{L}}{\\partial h}\n\\frac{\\partial h}{\\partial b}\n$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_linear_grad_W(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of\n",
    "        the loss with respect to W parameter of the function\n",
    "        dL / dW = (dL / dh) * (dh / dW)\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with\n",
    "            respect to the ouput of the dense layer (dL / dh)\n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss\n",
    "        with respect to W parameter of the function\n",
    "        np.array of size `(n_in, n_out)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(\n",
    "    username, w2_linear_grad_W, [\"x_input\", \"grad_output\", \"W\", \"b\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_linear_grad_b(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of\n",
    "        the loss with respect to b parameter of the function\n",
    "        dL / db = (dL / dh) * (dh / db)\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with\n",
    "            respect to the ouput of the linear function (dL / dh)\n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss\n",
    "        with respect to b parameter of the linear function\n",
    "        np.array of size `(n_out,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(\n",
    "    username, w2_linear_grad_b, [\"x_input\", \"grad_output\", \"W\", \"b\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.2 Sigmoid\n\nThe **sigmoid function** squashes values to the range [0, 1]:\n\n$$\nf = \\sigma(h) = \\frac{1}{1 + e^{-h}} \n$$\n\n**Properties:**\n- Applied **element-wise** (each element independently)\n- Doesn't change tensor dimensionality\n- Shape-agnostic implementation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_sigmoid_forward(x_input):\n",
    "    \"\"\"sigmoid nonlinearity\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "    # Output\n",
    "        the output of sigmoid layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w2_sigmoid_forward, [\"x_input\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 🔙 Sigmoid Gradient (Backpropagation)\n\nCompute the gradient of the loss with respect to sigmoid input:\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial h} = \n\\frac{\\partial \\mathcal{L}}{\\partial f}\n\\frac{\\partial f}{\\partial h} \n$$\n\n**Deriving** $\\frac{\\partial f}{\\partial h}$:\n\n$$\n\\frac{\\partial f}{\\partial h} = \n\\frac{\\partial \\sigma(h)}{\\partial h} =\n\\frac{\\partial}{\\partial h} \\Big(\\frac{1}{1 + e^{-h}}\\Big)\n= \\frac{e^{-h}}{(1 + e^{-h})^2}\n$$\n\n$$\n= \\frac{1}{1 + e^{-h}} \\cdot \\frac{e^{-h}}{1 + e^{-h}}\n= f(h) \\cdot (1 - f(h))\n$$\n\n**Implementation steps:**\n1. Calculate $f(h) \\cdot (1 - f(h))$ \n2. Multiply element-wise by $\\frac{\\partial \\mathcal{L}}{\\partial f}$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_sigmoid_grad_input(x_input, grad_output):\n",
    "    \"\"\"sigmoid nonlinearity gradient.\n",
    "        Calculate the partial derivative of the loss\n",
    "        with respect to the input of the layer\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "        grad_output: np.array of size `(n_objects, n_in)`\n",
    "            dL / df\n",
    "    # Output\n",
    "        the partial derivative of the loss\n",
    "        with respect to the input of the function\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "        dL / dh\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w2_sigmoid_grad_input, [\"x_input\", \"grad_output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3 Negative Log Likelihood (NLL)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Loss function:**\n\n$$\n\\mathcal{L} \n= -\\frac{1}{N}\\sum_j \\Big[ Y_j\\log \\dot{Y}_j + (1-Y_j)\\log(1-\\dot{Y}_j)\\Big]\n$$\n\nwhere:\n- $N$ = number of objects\n- $Y_j$ = true label\n- $\\dot{Y}_j$ = predicted label"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_nll_forward(target_pred, target_true):\n",
    "    \"\"\"Compute the value of NLL\n",
    "        for a given prediction and the ground truth\n",
    "    # Arguments\n",
    "        target_pred: predictions - np.array of size `(n_objects, 1)`\n",
    "        target_true: ground truth - np.array of size `(n_objects, 1)`\n",
    "    # Output\n",
    "        the value of NLL for a given prediction and the ground truth\n",
    "        scalar\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w2_nll_forward, [\"target_pred\", \"target_true\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 🔙 NLL Gradient\n\nCalculate the partial derivative of NLL with respect to predictions:\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}}\n=\n\\begin{pmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}_0} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}_1} \\\\\n\\vdots \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}_N}\n\\end{pmatrix}\n$$\n\n**Derivation** (step-by-step for component 0):\n\n$$\n\\begin{align}\n\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}_0} \n&= \\frac{\\partial}{\\partial \\dot{Y}_0} \\Big(-\\frac{1}{N}\\sum_j \\Big[ Y_j\\log \\dot{Y}_j + (1-Y_j)\\log(1-\\dot{Y}_j)\\Big]\\Big) \\\\\n&= -\\frac{1}{N} \\frac{\\partial}{\\partial \\dot{Y}_0} \\Big(Y_0\\log \\dot{Y}_0 + (1-Y_0)\\log(1-\\dot{Y}_0)\\Big) \\\\\n&= -\\frac{1}{N} \\Big(\\frac{Y_0}{\\dot{Y}_0} - \\frac{1-Y_0}{1-\\dot{Y}_0}\\Big) \\\\\n&= \\frac{1}{N} \\frac{\\dot{Y}_0 - Y_0}{\\dot{Y}_0 (1 - \\dot{Y}_0)}\n\\end{align}\n$$\n\n**General form** (element-wise operations):\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}} = \\frac{1}{N} \\frac{\\dot{Y} - Y}{\\dot{Y} \\odot (1 - \\dot{Y})}\n$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_nll_grad_input(target_pred, target_true):\n",
    "    \"\"\"Compute the partial derivative of NLL\n",
    "        with respect to its input\n",
    "    # Arguments\n",
    "        target_pred: predictions - np.array of size `(n_objects, 1)`\n",
    "        target_true: ground truth - np.array of size `(n_objects, 1)`\n",
    "    # Output\n",
    "        the partial derivative\n",
    "        of NLL with respect to its input\n",
    "        np.array of size `(n_objects, 1)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w2_nll_grad_input, [\"target_pred\", \"target_true\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.4 Model\n\nBelow is a complete **LogisticRegressionGD** model using the functions you implemented above."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogsticRegressionGD(object):\n",
    "\n",
    "    def __init__(self, n_in, lr=0.05):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.b = np.zeros(\n",
    "            1,\n",
    "        )\n",
    "        self.W = np.random.randn(n_in, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h = w2_linear_forward(x, self.W, self.b)\n",
    "        y = w2_sigmoid_forward(self.h)\n",
    "        return y\n",
    "\n",
    "    def update_params(self, x, nll_grad):\n",
    "        # compute gradients\n",
    "        grad_h = w2_sigmoid_grad_input(self.h, nll_grad)\n",
    "        grad_W = w2_linear_grad_W(x, grad_h, self.W, self.b)\n",
    "        grad_b = w2_linear_grad_b(x, grad_h, self.W, self.b)\n",
    "        # update params\n",
    "        self.W = self.W - self.lr * grad_W\n",
    "        self.b = self.b - self.lr * grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.5 Simple Experiment\n\nLet's test logistic regression on synthetic 2D datasets!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "def generate_2_circles(N=100):\n",
    "    phi = np.linspace(0.0, np.pi * 2, 100)\n",
    "    X1 = 1.1 * np.array([np.sin(phi), np.cos(phi)])\n",
    "    X2 = 3.0 * np.array([np.sin(phi), np.cos(phi)])\n",
    "    Y = np.concatenate([np.ones(N), np.zeros(N)]).reshape((-1, 1))\n",
    "    X = np.hstack([X1, X2]).T\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def generate_2_gaussians(N=100):\n",
    "    phi = np.linspace(0.0, np.pi * 2, 100)\n",
    "    X1 = np.random.normal(loc=[1, 2], scale=[2.5, 0.9], size=(N, 2))\n",
    "    X1 = X1 @ np.array([[0.7, -0.7], [0.7, 0.7]])\n",
    "    X2 = np.random.normal(loc=[-2, 0], scale=[1, 1.5], size=(N, 2))\n",
    "    X2 = X2 @ np.array([[0.7, 0.7], [-0.7, 0.7]])\n",
    "    Y = np.concatenate([np.ones(N), np.zeros(N)]).reshape((-1, 1))\n",
    "    X = np.vstack([X1, X2])\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def split(X, Y, train_ratio=0.7):\n",
    "    size = len(X)\n",
    "    train_size = int(size * train_ratio)\n",
    "    indices = np.arange(size)\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "    return X[train_indices], Y[train_indices], X[test_indices], Y[test_indices]\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "\n",
    "X, Y = generate_2_circles()\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=Y.ravel(), edgecolors=\"none\")\n",
    "ax1.set_aspect(\"equal\")\n",
    "\n",
    "\n",
    "X, Y = generate_2_gaussians()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=Y.ravel(), edgecolors=\"none\")\n",
    "ax2.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = split(*generate_2_circles(), 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train our model\n",
    "model = LogsticRegressionGD(2, 0.05)\n",
    "\n",
    "for step in range(100):\n",
    "    Y_pred = model.forward(X_train)\n",
    "\n",
    "    loss_value = w2_nll_forward(Y_pred, Y_train)\n",
    "    accuracy = ((Y_pred > 0.5) == Y_train).mean()\n",
    "    print(\n",
    "        \"Step: {} \\t Loss: {:.3f} \\t Acc: {:.1f}%\".format(\n",
    "            step, loss_value, accuracy * 100\n",
    "        )\n",
    "    )\n",
    "\n",
    "    loss_grad = w2_nll_grad_input(Y_pred, Y_train)\n",
    "    model.update_params(X_train, loss_grad)\n",
    "\n",
    "\n",
    "print(\"\\n\\nTesting...\")\n",
    "Y_test_pred = model.forward(X_test)\n",
    "test_accuracy = ((Y_test_pred > 0.5) == Y_test).mean()\n",
    "print(\"Acc: {:.1f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_prediction(prediction_func, X, Y, hard=True):\n",
    "    u_min = X[:, 0].min() - 1\n",
    "    u_max = X[:, 0].max() + 1\n",
    "    v_min = X[:, 1].min() - 1\n",
    "    v_max = X[:, 1].max() + 1\n",
    "\n",
    "    U, V = np.meshgrid(np.linspace(u_min, u_max, 100), np.linspace(v_min, v_max, 100))\n",
    "    UV = np.stack([U.ravel(), V.ravel()]).T\n",
    "    c = prediction_func(UV).ravel()\n",
    "    if hard:\n",
    "        c = c > 0.5\n",
    "    plt.scatter(UV[:, 0], UV[:, 1], c=c, edgecolors=\"none\", alpha=0.15)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y.ravel(), edgecolors=\"black\")\n",
    "    plt.xlim(left=u_min, right=u_max)\n",
    "    plt.ylim(bottom=v_min, top=v_max)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_model_prediction(lambda x: model.forward(x), X_train, Y_train, False)\n",
    "\n",
    "plot_model_prediction(lambda x: model.forward(x), X_train, Y_train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the same experiment on 2 circles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Nearest Neighbors\n\n---\n\n### 🎯 Introduction\n\n**k-Nearest Neighbors (k-NN)** is a **non-parametric** algorithm:\n- Unlike Logistic Regression, there are **no trainable parameters**\n- Classification based on similarity to training examples\n- Predicts labels by \"voting\" among k nearest neighbors\n\n**Algorithm:**\n1. Find the k training samples most similar to the input\n2. Predict the most frequent label among these k neighbors\n\n**For this assignment:** We implement the simple case where **k = 1** (single nearest neighbor)."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.1 Distance to Training Samples\n\nTo find the nearest neighbor, we first compute distances from the input to all training samples.\n\n**Distance metric:** [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)\n\n$$\nd(x, y) = \\sqrt{\\sum_{i} (x_i - y_i)^2}\n$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_dist_to_training_samples(x_input, training_set):\n",
    "    \"\"\"Calculate distance between an input sample and the N training samples.\n",
    "    # Arguments\n",
    "        x_input: samples for which we want to make a predicton\n",
    "            np.array of size `(n_in,)`\n",
    "        training_set: all our training samples\n",
    "            np.array of size `(N, n_in)`\n",
    "    # Output\n",
    "        The distances between our input samples and training samples\n",
    "        np.array of size `(N,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(\n",
    "    username, w2_dist_to_training_samples, [\"x_input\", \"training_set\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2 Predicting a Label\n\n**Steps:**\n1. Find the nearest neighbor (training sample with minimum distance)\n2. Predict by taking the label of this nearest neighbor"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_nearest_neighbors(distances, training_labels):\n",
    "    \"\"\"Predict the label of the input sample given the distances of\n",
    "        this sample to the training samples and the labels of the\n",
    "        training samples.\n",
    "    # Arguments\n",
    "        distances: distances from the input sample to the N training samples\n",
    "            np.array of size `(N,)`\n",
    "        training_labels: true labels of the training samples\n",
    "            np.array of size `(N,)`\n",
    "    # Output\n",
    "        prediction:\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(\n",
    "    username, w2_nearest_neighbors, [\"distances\", \"training_labels\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.3 Experiment\n\nLet's test the nearest neighbor method on our toy dataset."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors_pred_label(x_input, training_set, training_labels):\n",
    "    distances = w2_dist_to_training_samples(x_input, training_set)\n",
    "    return w2_nearest_neighbors(distances, training_labels)\n",
    "\n",
    "\n",
    "Y_pred = np.apply_along_axis(\n",
    "    lambda x: nearest_neighbors_pred_label(x, X_train, Y_train), axis=1, arr=X_test\n",
    ")\n",
    "accuracy = ((Y_pred > 0.5) == Y_test).mean()\n",
    "\n",
    "print(f\"{100 * accuracy:.1f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 💭 Discussion Question\n\n**Why is there a large performance difference between logistic regression and nearest neighbors on the 2 circles dataset?**\n\nThink about:\n- What kind of decision boundaries can logistic regression learn?\n- What kind of decision boundaries can nearest neighbors create?\n- Which is better suited for circular patterns?"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Decision Tree\n\n---\n\n### 🌳 Introduction\n\n**Decision Trees** are **non-parametric** models (like k-NN):\n- No trainable parameters to learn\n- Create hierarchical decision rules\n- Easy to interpret and visualize\n\n**Example:** Credit Decision Tree\n\n![Credit Decision Tree](images/creditdecisiontree.png)\n\n**How it works:**\n- Each node (except leaves) asks a question about features\n- Navigate from root to leaf based on feature values\n- Leaf nodes provide final predictions\n\n**Features in the credit example:**\n1. Checking account balance\n2. Duration of requested credit\n3. Payment status of previous loan\n4. Length of current employment"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 🔨 Building a Decision Tree\n\n**Construction algorithm** (starting from root node):\n\n1. **Choose splitting criterion** - Select best feature and threshold\n2. **Split dataset** - Divide into two groups based on criterion\n3. **Add child nodes** - One for each split\n4. **For each child, decide:**\n   - **A:** Repeat from step 1 (create another split)\n   - **B:** Make it a leaf node (predict by majority vote)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.1 Entropy & Data Split\n\n**Choosing good splitting rules:**\n\nTwo key criteria:\n1. **Informativeness** - Does the rule help make decisions?\n2. **Generalizability** - Will it work on unseen data?\n\nFollowing [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor): simpler is better.\n\n**Quality measure:** Weighted Entropy\n\n$$\nE(S) = \\sum_{i\\in \\{L, R\\}} \\frac{|S_i|}{|S|} E(S_i)\n$$\n\nwhere $S_L$ and $S_R$ are left and right splits, and:\n\n$$\nE(S_i) = -\\sum_{j=1}^n P_j \\log_2 (P_j)\n$$\n\n- $P_j$ = fraction of class j in the split\n- $n$ = number of classes (for binary: n = 2)\n- **Lower entropy = better split**\n- **Perfect split:** Entropy = 0 (all labels identical in each split)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_tree_weighted_entropy(Y_left, Y_right, classes):\n",
    "    \"\"\"Compute the weighted entropy.\n",
    "    # Arguments\n",
    "        Y_left: class labels of the data left set\n",
    "            np.array of size `(n_objects, 1)`\n",
    "        Y_right: class labels of the data right set\n",
    "            np.array of size `(n_objects, 1)`\n",
    "        classes: list of all class values\n",
    "    # Output\n",
    "        weighted_entropy: scalar `float`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return weighted_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(\n",
    "    username, w2_tree_weighted_entropy, [\"Y_left\", \"Y_right\", \"classes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 📂 Splitting Data\n\nAt each node, data is partitioned by a split criterion:\n- **Left child:** Examples where feature < split_value\n- **Right child:** Examples where feature ≥ split_value\n\nImplement functions to return the appropriate subset for each child."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_tree_split_data_left(X, Y, feature_index, split_value):\n",
    "    \"\"\"Split the data `X` and `Y`, at the feature indexed by `feature_index`.\n",
    "    If the value is less than `split_value` then return it as part of the left group.\n",
    "\n",
    "    # Arguments\n",
    "        X: np.array of size `(n_objects, n_in)`\n",
    "        Y: np.array of size `(n_objects, 1)`\n",
    "        feature_index: index of the feature to split at\n",
    "        split_value: value to split between\n",
    "    # Output\n",
    "        (XY_left): np.array of size `(n_objects_left, n_in + 1)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return XY_left\n",
    "\n",
    "\n",
    "def w2_tree_split_data_right(X, Y, feature_index, split_value):\n",
    "    \"\"\"Split the data `X` and `Y`, at the feature indexed by `feature_index`.\n",
    "    If the value is greater or equal than `split_value` then return it as part of the right group.\n",
    "\n",
    "    # Arguments\n",
    "        X: np.array of size `(n_objects, n_in)`\n",
    "        Y: np.array of size `(n_objects, 1)`\n",
    "        feature_index: index of the feature to split at\n",
    "        split_value: value to split between\n",
    "    # Output\n",
    "        (XY_left): np.array of size `(n_objects_left, n_in + 1)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return XY_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(\n",
    "    username, w2_tree_split_data_left, [\"X\", \"Y\", \"feature_index\", \"split_value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(\n",
    "    username, w2_tree_split_data_right, [\"X\", \"Y\", \"feature_index\", \"split_value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 🔍 Finding the Best Split\n\nWe search for the split rule with lowest weighted entropy by:\n- Trying all features\n- Trying all possible split values\n- Computing entropy for each combination\n- Selecting the minimum"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_best_split(X, Y):\n",
    "    class_values = list(set(Y.flatten().tolist()))\n",
    "    r_index, r_value, r_score = float(\"inf\"), float(\"inf\"), float(\"inf\")\n",
    "    r_XY_left, r_XY_right = (X, Y), (X, Y)\n",
    "    for feature_index in range(X.shape[1]):\n",
    "        for row in X:\n",
    "            XY_left = w2_tree_split_data_left(X, Y, feature_index, row[feature_index])\n",
    "            XY_right = w2_tree_split_data_right(X, Y, feature_index, row[feature_index])\n",
    "            XY_left, XY_right = (XY_left[:, :-1], XY_left[:, -1:]), (\n",
    "                XY_right[:, :-1],\n",
    "                XY_right[:, -1:],\n",
    "            )\n",
    "            entropy = w2_tree_weighted_entropy(XY_left[1], XY_right[1], class_values)\n",
    "            if entropy < r_score:\n",
    "                r_index, r_value, r_score = feature_index, row[feature_index], entropy\n",
    "                r_XY_left, r_XY_right = XY_left, XY_right\n",
    "    return {\n",
    "        \"index\": r_index,\n",
    "        \"value\": r_value,\n",
    "        \"XY_left\": r_XY_left,\n",
    "        \"XY_right\": r_XY_right,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.2 Terminal Node (Leaf)\n\n**Leaf nodes predict labels** by majority voting:\n- Take all training labels that reached this leaf\n- Return the most frequent label as the prediction"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_tree_to_terminal(Y):\n",
    "    \"\"\"The most frequent class label, out of the data points belonging to the leaf node,\n",
    "    is selected as the predicted class.\n",
    "\n",
    "    # Arguments\n",
    "        Y: np.array of size `(n_objects,1)`\n",
    "\n",
    "    # Output\n",
    "        label: most frequent label of `Y.dtype`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w2_tree_to_terminal, [\"Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.3 Build the Decision Tree\n\nRecursively build the tree by greedily splitting at each node based on entropy.\n\n**Preventing overfitting** - Convert to terminal node if:\n1. **Maximum depth** is reached\n2. Node has fewer than **minimum samples**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_recursive_split(X, Y, node, max_depth, min_size, depth):\n",
    "    XY_left, XY_right = node[\"XY_left\"], node[\"XY_right\"]\n",
    "    del node[\"XY_left\"]\n",
    "    del node[\"XY_right\"]\n",
    "    # check for a no split\n",
    "    if XY_left[0].size <= 0 or XY_right[0].size <= 0:\n",
    "        node[\"left_child\"] = node[\"right_child\"] = w2_tree_to_terminal(\n",
    "            np.concatenate((XY_left[1], XY_right[1]))\n",
    "        )\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node[\"left_child\"], node[\"right_child\"] = w2_tree_to_terminal(\n",
    "            XY_left[1]\n",
    "        ), w2_tree_to_terminal(XY_right[1])\n",
    "        return\n",
    "    # process left child\n",
    "    if XY_left[0].shape[0] <= min_size:\n",
    "        node[\"left_child\"] = w2_tree_to_terminal(XY_left[1])\n",
    "    else:\n",
    "        node[\"left_child\"] = tree_best_split(*XY_left)\n",
    "        tree_recursive_split(X, Y, node[\"left_child\"], max_depth, min_size, depth + 1)\n",
    "    # process right child\n",
    "    if XY_right[0].shape[0] <= min_size:\n",
    "        node[\"right_child\"] = w2_tree_to_terminal(XY_right[1])\n",
    "    else:\n",
    "        node[\"right_child\"] = tree_best_split(*XY_right)\n",
    "        tree_recursive_split(X, Y, node[\"right_child\"], max_depth, min_size, depth + 1)\n",
    "\n",
    "\n",
    "def build_tree(X, Y, max_depth, min_size):\n",
    "    root = tree_best_split(X, Y)\n",
    "    tree_recursive_split(X, Y, root, max_depth, min_size, 1)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 🖨️ Visualizing the Tree\n\nPrint split criteria or predicted classes at each node to understand decision-making.\n\n**Prediction:** Recursively traverse from root to leaf node based on feature values."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print(\"%s[X%d < %.3f]\" % ((depth * \" \", (node[\"index\"] + 1), node[\"value\"])))\n",
    "        print_tree(node[\"left_child\"], depth + 1)\n",
    "        print_tree(node[\"right_child\"], depth + 1)\n",
    "    else:\n",
    "        print(\"%s[%s]\" % ((depth * \" \", node)))\n",
    "\n",
    "\n",
    "def tree_predict_single(x, node):\n",
    "    if isinstance(node, dict):\n",
    "        if x[node[\"index\"]] < node[\"value\"]:\n",
    "            return tree_predict_single(x, node[\"left_child\"])\n",
    "        else:\n",
    "            return tree_predict_single(x, node[\"right_child\"])\n",
    "\n",
    "    return node\n",
    "\n",
    "\n",
    "def tree_predict_multi(X, node):\n",
    "    Y = np.array([tree_predict_single(row, node) for row in X])\n",
    "    return Y[:, None]  # size: (n_object,) -> (n_object, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 🧪 Testing the Decision Tree\n\nLet's test on toy data. **Note:** Ensure your `w2_tree_weighted_entropy` function handles empty splits correctly."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = split(*generate_2_circles(), 0.7)\n",
    "\n",
    "tree = build_tree(X_train, Y_train, 4, 1)\n",
    "Y_pred = tree_predict_multi(X_test, tree)\n",
    "test_accuracy = (Y_pred == Y_test).mean()\n",
    "print(\"Test Acc: {:.1f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Tree structure** printed in [pre-order traversal](https://en.wikipedia.org/wiki/Tree_traversal#Pre-order_(NLR)):"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_prediction(lambda x: tree_predict_multi(x, tree), X_test, Y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Experiments\n\n---\n\n### 🏥 Cleveland Heart Disease Dataset\n\nThe [Cleveland Heart Disease](https://archive.ics.uci.edu/ml/datasets/Heart+Disease) dataset predicts heart disease presence based on medical information.\n\n**Dataset details:**\n- Full database: 76 attributes\n- **We use: 14 key attributes**\n\n**Features (1-13):**\n\n1. **Age** - Age in years \n2. **Sex** - 0 = female, 1 = male \n3. **Chest pain type:**\n   - 1 = typical angina\n   - 2 = atypical angina\n   - 3 = non-anginal pain\n   - 4 = asymptomatic\n4. **Trestbps** - Resting blood pressure (mm Hg)\n5. **Chol** - Serum cholesterol (mg/dl) \n6. **Fasting blood sugar** - > 120 mg/dl (0 = false, 1 = true)\n7. **Resting ECG results:**\n   - 0 = normal\n   - 1 = ST-T wave abnormality\n   - 2 = left ventricular hypertrophy\n8. **Thalach** - Maximum heart rate achieved \n9. **Exercise induced angina** - 0 = no, 1 = yes\n10. **Oldpeak** - ST depression (exercise vs. rest) \n11. **Slope** - Peak exercise ST segment slope:\n    - 1 = upsloping, 2 = flat, 3 = downsloping \n12. **Ca** - Number of major vessels (0-3) colored by fluoroscopy \n13. **Thal:**\n    - 3 = normal, 6 = fixed defect, 7 = reversible defect \n\n**Target (14):**\n14. **Heart disease diagnosis** (angiographic disease status):\n    - 0 = < 50% diameter narrowing (no disease)\n    - 1 = > 50% diameter narrowing (disease present)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 📦 Data Preparation\n\nHelper functions for downloading and preprocessing are in `heart_disease_data.py`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heart_disease_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = heart_disease_data.download_and_preprocess()\n",
    "X_train, Y_train, X_test, Y_test = split(X, Y, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 👀 Exploring the Data\n\nLet's examine some examples:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0:2])\n",
    "print(Y_train[0:2])\n",
    "\n",
    "# TODO feel free to explore more examples and see if you can predict the presence of a heart disease"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.1 Decision Tree for Heart Disease Prediction\n\nLet's build a decision tree and evaluate performance!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: you are free to make use of code that we provide in previous cells\n",
    "# TODO: play around with different hyper parameters and see how these impact your performance\n",
    "\n",
    "tree = build_tree(X_train, Y_train, 5, 4)\n",
    "Y_pred = tree_predict_multi(X_test, tree)\n",
    "test_accuracy = (Y_pred == Y_test).mean()\n",
    "print(\"Test Acc: {:.1f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 💡 Hyperparameter Tuning\n\n**Question:** How did changing hyperparameters affect test performance?\n\n**Note:** Hyperparameters should be tuned using a [validation set](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets#Validation_dataset), not the test set, to avoid overfitting."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 Nearest Neighbors for Heart Disease Prediction\n\nTest k-NN with k=1 on the heart disease data:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.apply_along_axis(\n",
    "    lambda x: nearest_neighbors_pred_label(x, X_train, Y_train), axis=1, arr=X_test\n",
    ")\n",
    "accuracy = ((Y_pred > 0.5) == Y_test).mean()\n",
    "\n",
    "print(f\"{100 * accuracy:.1f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.3 Logistic Regression for Heart Disease Prediction\n\nTrain a logistic regression model to find correlations automatically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: you are free to make use of code that we provide in previous cells\n",
    "# TODO: play around with different hyper parameters and see how these impact your performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 📊 Performance Analysis\n\n**Question:** Was your model better than random guessing?\n\nLet's check the empirical mean of the target variable:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 🔍 Investigating the Problem\n\nLet's examine the learned parameters:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.W, model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 🎚️ Feature Scaling\n\n**Observation:** Some weights are much larger than others after training.\n\n**Analysis:**\n- Compare weight initialization range\n- Compare learning rate (step size)\n- Compare input feature scales\n\n**The problem:** Features have different scales!\n\n**Solution:** [Feature Scaling](https://en.wikipedia.org/wiki/Feature_scaling)\n\nRescale input features so they have similar ranges. This helps gradient descent converge faster and more reliably."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rescale the input features and train again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 💭 Final Discussion Question\n\n**Why didn't decision trees need feature scaling?**\n\nThink about how decision trees make splits versus how gradient descent optimizes parameters."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}